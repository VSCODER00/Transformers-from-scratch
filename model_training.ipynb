{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8130f4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0518e5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User ID</th>\n",
       "      <th>User Utterance</th>\n",
       "      <th>Bot Response</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Context/Session ID</th>\n",
       "      <th>Entities</th>\n",
       "      <th>User Feedback</th>\n",
       "      <th>Conversation Outcome</th>\n",
       "      <th>User Profile</th>\n",
       "      <th>Channel/Platform</th>\n",
       "      <th>Language</th>\n",
       "      <th>User Emotion/Sentiment</th>\n",
       "      <th>Location</th>\n",
       "      <th>User Segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uKqYhMMQ5S</td>\n",
       "      <td>Charge bar between follow student.</td>\n",
       "      <td>Important law into large example range. Player...</td>\n",
       "      <td>2023-10-31 18:02:06</td>\n",
       "      <td>4dfe56dc-efe2-49c6-be9f-ce5b84ca4de4</td>\n",
       "      <td>event</td>\n",
       "      <td>negative</td>\n",
       "      <td>incomplete</td>\n",
       "      <td>Annette Henderson</td>\n",
       "      <td>social media</td>\n",
       "      <td>German</td>\n",
       "      <td>confused</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>returning customers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YOonrpgxp9</td>\n",
       "      <td>Bad every reflect huge contain.</td>\n",
       "      <td>Policy argue agree character go recent. When r...</td>\n",
       "      <td>2023-10-31 18:02:06</td>\n",
       "      <td>74c97616-9df6-460d-991d-c6f6f6ae5354</td>\n",
       "      <td>location</td>\n",
       "      <td>neutral</td>\n",
       "      <td>specific outcome</td>\n",
       "      <td>Nicholas Haney</td>\n",
       "      <td>mobile app</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>London</td>\n",
       "      <td>returning customers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      User ID                      User Utterance  \\\n",
       "0  uKqYhMMQ5S  Charge bar between follow student.   \n",
       "1  YOonrpgxp9     Bad every reflect huge contain.   \n",
       "\n",
       "                                        Bot Response            Timestamp  \\\n",
       "0  Important law into large example range. Player...  2023-10-31 18:02:06   \n",
       "1  Policy argue agree character go recent. When r...  2023-10-31 18:02:06   \n",
       "\n",
       "                     Context/Session ID  Entities User Feedback  \\\n",
       "0  4dfe56dc-efe2-49c6-be9f-ce5b84ca4de4     event      negative   \n",
       "1  74c97616-9df6-460d-991d-c6f6f6ae5354  location       neutral   \n",
       "\n",
       "  Conversation Outcome       User Profile Channel/Platform Language  \\\n",
       "0           incomplete  Annette Henderson     social media   German   \n",
       "1     specific outcome     Nicholas Haney       mobile app  Chinese   \n",
       "\n",
       "  User Emotion/Sentiment Location         User Segment  \n",
       "0               confused   Sydney  returning customers  \n",
       "1             frustrated   London  returning customers  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('chatbot_dataset.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a77d7ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df['User Utterance']\n",
    "y=df['Bot Response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8bc93c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Important law into large example range. Player seem force with partner sometimes happen southern.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3881720c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Charge bar between follow student.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9354d467",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=[\"My name is Bablu\",\"Bablu goes to college\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32d980c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=100)\n",
    "tokenizer.fit_on_texts(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68a8d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences=tokenizer.texts_to_sequences(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f3a2580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3, 4, 1], [1, 5, 6, 7]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5533b7e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bablu': 1, 'my': 2, 'name': 3, 'is': 4, 'goes': 5, 'to': 6, 'college': 7}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45b4a8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 dimensions for each word\n",
    "import math\n",
    "def positionalEncodings(sequence):\n",
    "    \n",
    "    PE=[]\n",
    "    d_model=4\n",
    "    for pos in range(sequence):\n",
    "        EncodingsOfEach=[]\n",
    "        for i in range(d_model):\n",
    "            if i%2==0:\n",
    "                temp=pos/(math.pow(10000,(2*i)/d_model))\n",
    "                EncodingsOfEach.append(math.sin(temp))\n",
    "            else:\n",
    "                temp=pos/(math.pow(10000,(2*(i-1))/d_model))\n",
    "                EncodingsOfEach.append(math.cos(temp))\n",
    "        PE.append(EncodingsOfEach)\n",
    "    return PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54556273",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [\"The\", \"cat\", \"sat\"]\n",
    "PE=(positionalEncodings(len(tokens)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b302092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 1.0, 0.0, 1.0],\n",
       " [0.8414709848078965, 0.5403023058681398, 9.999999983333334e-05, 0.999999995],\n",
       " [0.9092974268256817,\n",
       "  -0.4161468365471424,\n",
       "  0.0001999999986666667,\n",
       "  0.9999999800000001]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37742188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[1,0,1,0]\n",
    "b=np.array([[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]])\n",
    "result=np.matmul(a,b)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8bbe099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "(4, 4)\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "a=np.array([1,0,1,0])\n",
    "b=np.array([[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]])\n",
    "result=np.matmul(a,b)\n",
    "result\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "print(result.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b2c26f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input will be the embedding(+PE) \n",
    "# eg:[[1,0,1,0],[0,1,0,1],[1,1,1,1]]\n",
    "def selfAttention(embeddings,W_q, W_k,W_v):\n",
    "    numberOfWords=len(embeddings)\n",
    "    Q_K_V_ofAllWords=[]\n",
    "    for i in range(numberOfWords):\n",
    "        Q_of_given_word=np.matmul(embeddings[i],W_q)\n",
    "        K_of_given_word=np.matmul(embeddings[i],W_k)\n",
    "        V_of_given_word=np.matmul(embeddings[i],W_v)\n",
    "        temp=[]\n",
    "        temp.append(Q_of_given_word)\n",
    "        temp.append(K_of_given_word)\n",
    "        temp.append(V_of_given_word)\n",
    "        Q_K_V_ofAllWords.append(temp)\n",
    "    d_k=math.sqrt(len(Q_K_V_ofAllWords[0][1]))\n",
    "    scoreOfAllWords=[]\n",
    "    for i in range(numberOfWords):\n",
    "        query=Q_K_V_ofAllWords[i][0]\n",
    "        scoreOfEachWord=[]\n",
    "        for j in range(numberOfWords):\n",
    "            key=Q_K_V_ofAllWords[j][1]\n",
    "            scoreOfEachWord.append(np.matmul(query,key)/d_k)\n",
    "        scoreOfAllWords.append(scoreOfEachWord)\n",
    "    attentionWeightsOfAllWords=[]\n",
    "    for i in range(numberOfWords):\n",
    "        attentionWeightsOfAllWords.append(softmax(scoreOfAllWords[i]))\n",
    "    \n",
    "    finalAttentionOutput=[]\n",
    "    for i in range(numberOfWords):\n",
    "        d_v = len(Q_K_V_ofAllWords[0][2])\n",
    "        temp = np.zeros(d_v)\n",
    "        for j in range(numberOfWords):\n",
    "            temp += attentionWeightsOfAllWords[i][j] * Q_K_V_ofAllWords[j][2]\n",
    "        finalAttentionOutput.append(temp)\n",
    "    return finalAttentionOutput\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c557ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_matrix=np.array([[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]])\n",
    "embeddings=np.array([[1,0,1,0],[0,1,0,1],[1,1,1,1]])\n",
    "self_atten_out=selfAttention(embeddings,temp_matrix,temp_matrix,temp_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9057e465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.8446376, 0.5776812, 0.8446376, 0.5776812]),\n",
       " array([0.5776812, 0.8446376, 0.5776812, 0.8446376]),\n",
       " array([0.78805844, 0.78805844, 0.78805844, 0.78805844])]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_atten_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0465a4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Add_and_normalize(output_of_previous,input_of_previous):\n",
    "    #added=output_of_previous+input_of_previous\n",
    "    output=[]\n",
    "    eps = 1e-6\n",
    "    for i in range(len(output_of_previous)):\n",
    "        added=output_of_previous[i]+input_of_previous[i]\n",
    "        mean = np.mean(added)\n",
    "        std_dev = np.std(added)\n",
    "        normalized_array=(added-mean)/(std_dev+eps)\n",
    "        output.append(normalized_array)\n",
    "    return output\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c78475c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_and_normalize_1_out=(Add_and_normalize(self_atten_out,embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "12b6e166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.99999842, -0.99999842,  0.99999842, -0.99999842]),\n",
       " array([-0.99999842,  0.99999842, -0.99999842,  0.99999842]),\n",
       " array([0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_and_normalize_1_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fe70a8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeedForwardNN(x,W1,b1,W2,b2):\n",
    "    output=[]\n",
    "    for i in range(len(x)):\n",
    "        hidden=np.maximum(0,np.matmul(x[i],W1)+b1)\n",
    "        out=np.matmul(hidden,W2)+b2\n",
    "        output.append(out)\n",
    "    return output\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9d80c521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([74.44746168, 76.26869145, 74.69544957, 73.8961129 ]),\n",
       " array([83.22672236, 81.79718994, 88.32028904, 80.69721868]),\n",
       " array([67.6595549 , 69.42100484, 71.3437252 , 65.49451367])]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimOfInput=add_and_normalize_1_out[0].shape[0]\n",
    "W1=np.random.random((dimOfInput,256))\n",
    "b1=np.random.random((256,))\n",
    "W2=np.random.random((256,dimOfInput))\n",
    "b2=np.random.random((dimOfInput,))\n",
    "ffn_out=FeedForwardNN(add_and_normalize_1_out,W1,b1,W2,b2)\n",
    "ffn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "78419d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.55157821,  0.39267561,  0.77200986, -1.71626368]),\n",
       " array([-0.57464065, -0.31926194,  1.70558656, -0.81168396]),\n",
       " array([-0.37959203,  0.43566937,  1.32557244, -1.38164978])]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_out=Add_and_normalize(ffn_out,add_and_normalize_1_out)\n",
    "encoder_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c464b2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Encoder(positional_encodings_plus_embeddings,W_q,W_k,W_v,W1,b1,W2,b2):\n",
    "    self_attn_output=selfAttention(positional_encodings_plus_embeddings,W_q,W_k,W_v)\n",
    "    add_and_normalize_1_output=Add_and_normalize(self_attn_output,positional_encodings_plus_embeddings)\n",
    "    ffn_output=FeedForwardNN(add_and_normalize_1_output,W1,b1,W2,b2)\n",
    "    encoder_output=Add_and_normalize(ffn_output,add_and_normalize_1_output)\n",
    "    return encoder_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3ff6bd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=np.array([[1,0,1,0],[0,1,0,1],[1,1,1,1]])\n",
    "positional_encodings=positionalEncodings(len(embeddings))\n",
    "embeddings_plus_PE=embeddings+positional_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9d150a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = embeddings_plus_PE.shape[1]\n",
    "\n",
    "W_q = np.eye(d_model)\n",
    "W_k = np.eye(d_model)\n",
    "W_v = np.eye(d_model)\n",
    "\n",
    "W1 = np.random.randn(d_model, 256)\n",
    "b1 = np.zeros(256)\n",
    "W2 = np.random.randn(256, d_model)\n",
    "b2 = np.zeros(d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d4ad2ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 0.4625303 ,  0.8084736 ,  0.44262381, -1.71362771]), array([-0.35390072,  0.59077209,  1.20400299, -1.44087436]), array([ 0.59019507,  1.05767249, -0.05736201, -1.59050555])]\n"
     ]
    }
   ],
   "source": [
    "print(Encoder(embeddings_plus_PE,W_q,W_k,W_v,W1,b1,W2,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b1c627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AnnProject2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
