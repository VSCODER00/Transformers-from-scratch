{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96ae3ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8d10464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 dimensions for each word\n",
    "import math\n",
    "def positionalEncodings(sequence):\n",
    "    \n",
    "    PE=[]\n",
    "    d_model=4\n",
    "    for pos in range(sequence):\n",
    "        EncodingsOfEach=[]\n",
    "        for i in range(d_model):\n",
    "            if i%2==0:\n",
    "                temp=pos/(math.pow(10000,(2*i)/d_model))\n",
    "                EncodingsOfEach.append(math.sin(temp))\n",
    "            else:\n",
    "                temp=pos/(math.pow(10000,(2*(i-1))/d_model))\n",
    "                EncodingsOfEach.append(math.cos(temp))\n",
    "        PE.append(EncodingsOfEach)\n",
    "    return PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48fff696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selfAttention(embeddings, W_q, W_k, W_v):\n",
    "    numberOfWords = tf.shape(embeddings)[0]\n",
    "\n",
    "    Q, K, V = [], [], []\n",
    "    for i in range(numberOfWords):\n",
    "        x_i = tf.expand_dims(embeddings[i], 0)\n",
    "        Q.append(tf.matmul(x_i, W_q))\n",
    "        K.append(tf.matmul(x_i, W_k))\n",
    "        V.append(tf.matmul(x_i, W_v))\n",
    "\n",
    "    d_k = tf.math.sqrt(tf.cast(tf.shape(K[0])[1], tf.float32))\n",
    "\n",
    "    attention_outputs = []\n",
    "\n",
    "    for i in range(numberOfWords):\n",
    "        score_list = []\n",
    "        for j in range(numberOfWords):\n",
    "            score = tf.matmul(Q[i], K[j], transpose_b=True)[0, 0] / d_k\n",
    "            score_list.append(score)\n",
    "\n",
    "        scores = tf.stack(score_list)\n",
    "        weights = tf.nn.softmax(scores)\n",
    "\n",
    "        output = tf.zeros_like(V[0])\n",
    "        for j in range(numberOfWords):\n",
    "            output += weights[j] * V[j]\n",
    "\n",
    "        attention_outputs.append(output[0])\n",
    "\n",
    "    return tf.stack(attention_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adc88dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Add_and_normalize(output_of_previous,input_of_previous):\n",
    "    layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    return layer_norm(output_of_previous + input_of_previous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea40a037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeedForwardNN(x, W1, b1, W2, b2):\n",
    "    hidden = tf.nn.relu(tf.matmul(x, W1) + b1)\n",
    "    out = tf.matmul(hidden, W2) + b2\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c5f8b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Encoder(positional_encodings_plus_embeddings,W_q,W_k,W_v,W1,b1,W2,b2):\n",
    "    self_attn_output=selfAttention(positional_encodings_plus_embeddings,W_q,W_k,W_v)\n",
    "    add_and_normalize_1_output=Add_and_normalize(self_attn_output,positional_encodings_plus_embeddings)\n",
    "    ffn_output=FeedForwardNN(add_and_normalize_1_output,W1,b1,W2,b2)\n",
    "    encoder_output=Add_and_normalize(ffn_output,add_and_normalize_1_output)\n",
    "    return encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ae8bf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=tf.constant([[1,0,1,0],[0,1,0,1],[1,1,1,1]],dtype=tf.float32)\n",
    "positional_encodings=positionalEncodings(len(embeddings))\n",
    "embeddings_plus_PE=embeddings+positional_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2b616c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model=embeddings_plus_PE[0].shape[0]\n",
    "W_q=tf.constant([[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]],dtype=tf.float32)\n",
    "W_k=tf.constant([[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]],dtype=tf.float32)\n",
    "W_v=tf.constant([[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]],dtype=tf.float32)\n",
    "W1 = tf.random.normal(shape=(d_model,256))\n",
    "b1 = tf.random.normal(shape=(256,))\n",
    "W2 = tf.random.normal(shape=(256,d_model))\n",
    "b2 = tf.random.normal(shape=(d_model,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74ba508f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.25423634 -1.6654139   0.9937296   0.4174478 ]\n",
      " [ 0.21969035 -1.6718075   0.499704    0.95241326]\n",
      " [ 0.18654864 -1.5957975   1.1659479   0.24330103]], shape=(3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(Encoder(embeddings_plus_PE,W_q,W_k,W_v,W1,b1,W2,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c708cbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model=embeddings_plus_PE[0].shape[0]\n",
    "encoder_params = []\n",
    "for _ in range(6):\n",
    "    params = {\n",
    "        \"W_q\": tf.random.normal(shape=(d_model,d_model)),\n",
    "        \"W_k\": tf.random.normal(shape=(d_model,d_model)),\n",
    "        \"W_v\": tf.random.normal(shape=(d_model,d_model)),\n",
    "        \"W1\": tf.random.normal(shape=(d_model,256)),\n",
    "        \"b1\": tf.random.normal(shape=(256,)),\n",
    "        \"W2\": tf.random.normal(shape=(256,d_model)),\n",
    "        \"b2\": tf.random.normal(shape=(d_model,))\n",
    "    }\n",
    "    encoder_params.append(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e386143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoderStack(x, encoder_params):\n",
    "    for layer in encoder_params:\n",
    "        x = Encoder(\n",
    "            x,\n",
    "            layer[\"W_q\"],\n",
    "            layer[\"W_k\"],\n",
    "            layer[\"W_v\"],\n",
    "            layer[\"W1\"],\n",
    "            layer[\"b1\"],\n",
    "            layer[\"W2\"],\n",
    "            layer[\"b2\"]\n",
    "        )\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1364aedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output=(encoderStack(embeddings_plus_PE,encoder_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d25b8183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[ 1.7191175 , -0.41607088, -0.75771856, -0.5453281 ],\n",
       "       [ 1.7028332 , -0.3062513 , -0.82348776, -0.5730942 ],\n",
       "       [ 1.6999527 , -0.28766042, -0.82888055, -0.5834118 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b8df09",
   "metadata": {},
   "source": [
    "Decoder:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a7cfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_Attention(embeddings, W_q, W_k, W_v,mask):\n",
    "    numberOfWords = tf.shape(embeddings)[0]\n",
    "\n",
    "    Q, K, V = [], [], []\n",
    "    for i in range(numberOfWords):\n",
    "        x_i = tf.expand_dims(embeddings[i], 0)\n",
    "        Q.append(tf.matmul(x_i, W_q))\n",
    "        K.append(tf.matmul(x_i, W_k))\n",
    "        V.append(tf.matmul(x_i, W_v))\n",
    "\n",
    "    d_k = tf.math.sqrt(tf.cast(tf.shape(K[0])[1], tf.float32))\n",
    "\n",
    "    attention_outputs = []\n",
    "\n",
    "    for i in range(numberOfWords):\n",
    "        score_list = []\n",
    "        for j in range(numberOfWords):\n",
    "            score = tf.matmul(Q[i], K[j], transpose_b=True)[0, 0] / d_k\n",
    "            score=score+mask[i][j]\n",
    "            score_list.append(score)\n",
    "\n",
    "        scores = tf.stack(score_list)\n",
    "        weights = tf.nn.softmax(scores)\n",
    "\n",
    "        output = tf.zeros_like(V[0])\n",
    "        for j in range(numberOfWords):\n",
    "            output += weights[j] * V[j]\n",
    "\n",
    "        attention_outputs.append(output[0])\n",
    "\n",
    "    return tf.stack(attention_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c69282d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Encoder_decoder_attention(encoder_output,prev_layer_output,W_q,W_k,W_v):\n",
    "    numberOfWords = tf.shape(encoder_output)[0]\n",
    "\n",
    "    Q, K, V = [], [], []\n",
    "    for i in range(numberOfWords):\n",
    "        q_i = tf.expand_dims(prev_layer_output[i], 0)\n",
    "        enc_i = tf.expand_dims(encoder_output[i], 0)\n",
    "        Q.append(tf.matmul(q_i, W_q))\n",
    "        K.append(tf.matmul(enc_i, W_k))\n",
    "        V.append(tf.matmul(enc_i, W_v))\n",
    "\n",
    "    d_k = tf.math.sqrt(tf.cast(tf.shape(K[0])[1], tf.float32))\n",
    "\n",
    "    attention_outputs = []\n",
    "\n",
    "    for i in range(numberOfWords):\n",
    "        score_list = []\n",
    "        for j in range(numberOfWords):\n",
    "            score = tf.matmul(Q[i], K[j], transpose_b=True)[0, 0] / d_k\n",
    "            score_list.append(score)\n",
    "\n",
    "        scores = tf.stack(score_list)\n",
    "        weights = tf.nn.softmax(scores)\n",
    "\n",
    "        output = tf.zeros_like(V[0])\n",
    "        for j in range(numberOfWords):\n",
    "            output += weights[j] * V[j]\n",
    "\n",
    "        attention_outputs.append(output[0])\n",
    "\n",
    "    return tf.stack(attention_outputs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5130bd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(positional_encodings_plus_embeddings,encoder_output,W_q_self,W_k_self,W_v_self,W_q_cross,W_k_cross,W_v_cross,mask,W1,b1,W2,b2):\n",
    "    masked_self_attn_output=masked_Attention(positional_encodings_plus_embeddings,W_q_self,W_k_self,W_v_self,mask)\n",
    "    add_and_normalize_1_output=Add_and_normalize(masked_self_attn_output,positional_encodings_plus_embeddings)\n",
    "    encoder_decoder_attn_output=Encoder_decoder_attention(encoder_output,add_and_normalize_1_output,W_q_cross,W_k_cross,W_v_cross)\n",
    "    add_and_normalize_2_output=Add_and_normalize(encoder_decoder_attn_output,add_and_normalize_1_output)\n",
    "    ffn_output=FeedForwardNN(add_and_normalize_2_output,W1,b1,W2,b2)\n",
    "    decoder_output=Add_and_normalize(ffn_output,add_and_normalize_2_output)\n",
    "    return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6fa646e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_mask(d):\n",
    "    mask = tf.linalg.band_part(tf.ones((d, d)), -1, 0)\n",
    "    return (1.0 - mask) * -1e9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c0f5e881",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model=embeddings_plus_PE[0].shape[0]\n",
    "mask=causal_mask(d_model)\n",
    "decoder_params = []\n",
    "\n",
    "for _ in range(6):\n",
    "    params = {\n",
    "        \"W_q_self\": tf.random.normal(shape=(d_model,d_model)),\n",
    "        \"W_k_self\": tf.random.normal(shape=(d_model,d_model)),\n",
    "        \"W_v_self\": tf.random.normal(shape=(d_model,d_model)),\n",
    "        \"W_q_cross\": tf.random.normal(shape=(d_model,d_model)),\n",
    "        \"W_k_cross\": tf.random.normal(shape=(d_model,d_model)),\n",
    "        \"W_v_cross\": tf.random.normal(shape=(d_model,d_model)),\n",
    "        \"W1\": tf.random.normal(shape=(d_model,256)),\n",
    "        \"b1\": tf.random.normal(shape=(256,)),\n",
    "        \"W2\": tf.random.normal(shape=(256,d_model)),\n",
    "        \"b2\": tf.random.normal(shape=(d_model,))\n",
    "    }\n",
    "    decoder_params.append(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bd4c1600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoderStack(x,decoder_params,encoder_output):\n",
    "    for layer in decoder_params:\n",
    "        x=decoder(x,encoder_output,layer[\"W_q_self\"],layer[\"W_k_self\"],layer[\"W_v_self\"],layer[\"W_q_cross\"],layer[\"W_k_cross\"],layer[\"W_v_cross\"],mask,layer[\"W1\"],layer[\"b1\"],layer[\"W2\"],layer[\"b2\"])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d5e665",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=tf.constant([[1,0,1,0],[0,1,1,0],[1,1,0,1]],dtype=tf.float32)\n",
    "positional_encodings=positionalEncodings(len(embeddings))\n",
    "embeddings_plus_PE=embeddings+positional_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "60110059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-1.2151812   1.5449194   0.03785586 -0.367594  ]\n",
      " [-1.2151648   1.5449364   0.03781128 -0.3675828 ]\n",
      " [-1.2152151   1.5449104   0.03782833 -0.36752343]], shape=(3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(decoderStack(embeddings_plus_PE,decoder_params,encoder_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6e5ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AnnProject2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
